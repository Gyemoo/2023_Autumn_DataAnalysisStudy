# 1주차 과제
## 2.3 데이터 로드와 저장
### 2.3.1 데이터 로드
```python
df = pd.read_csv('파일 경로')
```
* csv파일을 pandas dataframe형식으로 변수에 저장함
* 일반적인 경우 csv파일의 컬럼명이 적용되며, 인덱스는 0부터 시작되는 숫자이다.
```python
df = pd.read_csv('파일 경로', header=None)
```
* 컬럼명이 없을 경우 첫번째 인덱스가 자동으로 컬럼명으로 적용되니, header=None을 입력하여야 한다.
```python
col_names = ['컬럼1', '컬럼2', '컬럼3','컬럼4','컬럼5','키워드']
pd.read_csv('파일 경로' , names = col_names, index_col = '키워드')
```
* 특정 컬럼의 값들을 인덱스로 지정할 수 있다. 키워드 컬럼의 값들이 순차적으로 인덱스로 지정됨
```python
pd.read_csv('파일 경로' , skiprows = lambda x: x>0 and np.random.rand() > 0.5)
```
* skiprows로 전달된 인자에 해당하는 rows를 생략하며, 위 코드를 통해 50%의 데이터만 추출할 수 있다.
```python
pd.isnull(df) # 결측값 Ture, 아니면 False
pd.read_csv('파일 경로' , na_values=['NaN','A','B'])
```
* na_values를 활용하여 특정 values를 NaN으로 치환 가능하다.
```python
pd.read_csv('파일 경로', encoding='cp949', engine='python')
```
* 파일을 읽어올 때 한글이 깨지는경우 encoding방식과 engine을 지정해주면 대부분 제대로 동작한다.
* 외우기보단 안되면 구글링해서 씁시다.

### 2.3.2 데이터프레임 출력
```python
pd.set_option("display.max_columns", 60) #
pd.set_option("display.max_rows", 150) #
pd.options.display.max_colums = 60
pd.options.display.max_rows = 30 # 똑같은 기능
```
* 기본적으로 데이터프레임을 생성하면 20x60의 형태로 보여주고, 더 큰 경우는 중략되지만, option을 활용하여 한번에 보이는 크기를 바꿔줄 수 있다.

### 2.3.3 데이터프레임 저장
```python
df.to_csv('파일 경로/이름.csv')
df.to_excel('파일 경로/이름.xlsx')
```
* 데이터프레임을 csv 파일 혹은 excel파일로 저장할 수 있으며, header, index, columns 매개변수를 활용하여 특정 값만 저장할 수 있다.

## 2.4 데이터 확인
### 2.4.1 데이터의 처음과 끝
```python
df.head(a)
df.tail(b)
```
* 데이터프레임의 위에서부터 a개, 아래서부터 b개를 확인할 수 있다. 기본값은 5이다.

### 2.4.2 데이터 차원과 길이 확인
```python
df.shape # (a,b) 형태로 출력, a행 b열인 데이터프레임
df.size # a*b, 총 데이터 개수 출력
len(df) # 데이터의 길이 = 행 수 출력
```

### 2.4.3 데이터타입 확인, 변경
```python
df = df.convert_dtypes()
```
* 처음 데이터를 읽었을 때 모호한 경우 object type으로 처리되는 경우가 많은데, convert_dtypes()함수를 통해 좀 더 정확하게 데이터타입을 얻을 수 있다.

```python
df = df.astype({'species' : 'category'})
df['species'] = df['species'].astype('category')
```
* astype()함수를 이용하여 데이터타입을 바꿀 수 있고, 두가지 방법을 사용 가능하다.
* 결측값이 있는 경우 에러가 발생할 수 있는데, 해당 행을 삭제하거나 적당한 값으로 채워넣어야 한다.

### 2.4.4 요약 정보 확인
```python
df.info()
```
* 행의 수, 각 행별 데이터의 수(결측값 제외), 데이터타입, 메모리 사용량 등을 볼 수 있다.

### 2.4.5 기술통계 확인
```python
df.describe() #여러가지 기술통계량 확인 가능
df.count() # col, row 기준 결측값이 아닌 데이터 개수
df.max() # 최댓값
df.min() # 최솟값
df.mean() # 평균값
df.std() # 표준편차
df.sum() # 데이터의 합
```
* 그 외에 seaborn 라이브러리를 이용하여 시각화 가능 (나중에 다룸)

### 2.4.6 고윳값 확인
```python
df.unique() # 고윳값 추출
df.value_counts(normalize=True) # 고윳값 개수 확인, normalize를 통해 비중도 확인 가능
```

## 2.5 컬럼 다루기
### 2.5.1 컬럼 목록
```python
df.columns #인덱스로 컬럼 값 반환
```
### 2.5.2 컬럼 호출, 조합
```python
df[['species'],'bill_depth_mm']] # 여러개 호출할 시 리스트 이중으로
df.species # 하나일 경우엔 위의 방식 혹은 .으로도 가능 ( 더 편리)
```

### 2.5.3 컬럼 생성하기
```python
df['bill_depth_cm'] = df['bill_depth_mm']/10 # 하나일 경우
df.assign(
'bill_depth_cm' = 'bill_depth_mm'/10
'bill_length_cm'] = 'bill_length_mm'/10
) # 여러개일 경우
```

### 2.5.4 동일한 데이터타입 선택
```python
df.select_dtypes(include=['float64']).columns
```
* 제외하고싶으면 exclude를 사용한다.

### 2.5.5 컬럼, 로우 삭제하기
```python
df = df.drop('species', axis=1)
```
* df = 을 붙여주어야 삭제한 값이 저장된다. axis=1은 세로를 뜻함. 여러 줄일때는 첫 매개변수에 리스트로 넣는다.

### 2.5.6. 컬럼 이름 변경하기
```python
df.rename(columns={"species" : "펭귄 종류"})
df.columns = ['펭귄 종류', ... , '성별']
```
* 위의 방법은 특정 컬럼을 지정하여 원하는 이름으로 바꿀 수 있고, 아래의 방법은 모든 column에 대해 값을 정해주어야 한다.

### 2.5.7 컬럼 순서/위치 변경하기
```python
df.columns = df.columns[[0,1,6,2,3,4,5]]
```
* 리스트형태로 모든 순서를 새로 지정할 수 있고, 컬럼명을 새로 배치하면 그대로 적용된다.

### 2.5.8 컬럼과 인덱스 교환
```python
df = df[:11].transpose()
df[:11].T
```
* 11줄까지만 선택하여 전치하는 코드. 위 아래의 실행 결과는 동일하다.

## 2.6 데이터 인덱싱
### 2.6.1 문자형 인덱스 인덱싱
> 인덱스가 숫자가 아닌 문자인 경우에 대해서 다룸
```python
df.loc[A,B] # A행 B열의 값 출력
df.loc[[A,B]] # A,B행의 값 출력
```
* loc함수로 특정 구간, 값을 지정한 후 = 로 대입하여 값을 바꿀 수 있다.

### 2.6.2 위치 기반 인덱싱
```python
df.iloc[[a]] # a 행의 값들을 데이터프레임으로 출력, 리스트를 한번만 감싸면 시리즈로 출력됨
df.iloc[[a,b]] # a,b 행 출력
```

### 2.6.3 컬럼을 인덱스로 만들기
```python
df.set_index('species', inplace=True)
df.reset_index(inplace=True)
```
* 특정 컬럼을 인덱스값으로 지정할 수 있고, reset_index를 하면 다시 숫자 인덱스로 돌아온다. inplace=True를 넣어줘야 df에 적용된다.

## 정리
캐글스터디를 할때 자세한 공부 없이 그냥 실사용만 했어서 원리나 각종 응용에 대해선 잘 배우지 못했었는데, 실제 코드를 작성하는 데에 도움이 될 것 같다. seaborn 라이브러리를 import하여 sns.load_dataset('iris')를 하면 기본 제공 데이터셋을 사용할 수 있어, 매번 코랩에 업로드하고 read_csv를 하여 데이터프레임 다루기 연습을 하는 것보단 훨씬 편리한 것 같다. 자주 이용해야겠다.

