4.3 데이터 병합 후 처리 ~ 5.1 그룹 연산의 이해
## **주요 내용**
- 합친 데이터에서 중복 행 확인 및 삭제하는 방법이다.
```python
# 데이터셋
data_d = pd.DataFrame({
	'패션아이템': ['팬츠', '팬츠', '자켓', '자켓', '자켓'],
	'스타일': ['캐주얼', '캐주얼', '캐주얼', '비즈니스룩', '비즈니스룩'],
	'선호도(5점 만점)': [4, 4, 3.5, 4.2, 2.7]
})
```
```python
# duplicated() 함수를 활용해 중복 행 유무의 결과로 bool 값을 반환한다.
data_d.duplicated(keep='first')
# keep은 중복 행이 있을 때 처음 행에 체크를 할 것인지 마지막 행에 체크를 할 것인지 결정하는 것이다.
# False는 중복되는 모든 행을 다 표시한다.
```
```python
# value_counts() 함수를 함께 활용하면 중복 행이 몇 개인지 파악할 수 있다.
data_d.duplicated(keep=False).value_counts()

# 매개변수 subset으로 특정 컬럼만 제한해서 중복 데이터를 파악할 수도 있다.
data_d.duplicated(subset=['스타일']).value_counts()
``` 
```python
# 중복 행 삭제는 drop_duplicates() 함수를 활용한다.
data_d.drop_duplicates()
data_d.drop_duplicates(subsets=['패션아이템', '스타일'], keep='last')
```

- 데이터 길이가 동일한 2개 데이터를 비교할 때는 compare () 함수를 사용하는데 어느 위치가 다른지 차이점을 보여준다.
```python
# 데이터셋 생성
data_c = pd.DataFrame({
	'패션아이템': ['팬츠', '팬츠', '자켓', '자켓', '팬츠'],
	'선호도': [1.0, 2.0, 3.0, np.nan, 5.0],
	'평점': [1.0, 2.0, 3.0, 4.0, 5.0]
})

data_c_2 = data_c.copy()
data_c_2.loc[0, '패션아이템'] = '스커트'
data_c_2.loc[2, '평점'] = 4.0
```
```python
# 값 차이 비교
data_c.compare(data_c_2)

# NaN 대신 원본 데이터 값 보기
data_c.compare(data_c_2, keep_equal=True)

# 세로축 변경
data_c.compare(data_c_2, align_axis=0)

# 전체 데이터 사이즈 유지한 채 비교
data_c.compare(data_c_2, keep_shape=True)
```

- 데이터 길이가 다른 데이터 비교
```python
# 데이터셋
data_e = pd.DataFrame({
	'패션아이템': ['팬츠', '스커트', '자켓', '티셔츠', '블라우스', '베스트'],
	'평점': [3.0, 5.0, 7.0, 5.0, 2.0, 4.0]
})

data_e_2 = pd.DataFrame({
	'패션아이템': ['팬츠', '스커트', '자켓', '티셔츠', '블라우스', '베스트', '패딩'],
	'평점': [3.0, 5.0, 7.0, 5.0, 2.0, 4.0, 8.0]
})
```
```python
# 2개 데이터 중 가장 긴 길이를 기준으로 값이 다른지 여부를 bool 타입으로 반환한다.
data_e.eq(data_e_2)

# 행의 어느 위치에라도 다른 값이 있는 경우 True를 반환
data_e.eq(data_e_2).all(axis=1)

# True에 해당하는 데이터만 필터링
data_e_2[data_e.eq(data_e_2).all(axis=1) == False]
```
```python
# 데이터셋 생성
data_e_3 = pd.DataFrame({
	'패션아이템': ['팬츠', '모자', '자켓', '패딩', '스카프', '장갑', '스커트'],
	'평점': [3.0, 6.0, 7.0, 6.0, 7.0, 3.0, 5.0]
})

# 인덱스가 다른 경우 동일한 값으로 인식시키기
name_new = data_e_3.rename({'평점': '평점_2차'}, axix=1)
data_merged = name_new.merge(data_e, how='left',
					left_on=['패션아이템', '평점_2차],
					right_on=['패션아이템', '평점'])
data_merged

# 결측값을 포함하고 있는 데이터를 추출
data_e_3[data_merged['평점'].isna()]
```

- grouby
```python
# 데이터셋 불러오기
diamonds = sns.load_dataset('diamonds')

# 평균 구하기
grouped = diamonds['price'].groupby(diamonds['clarity'])
grouped.mean()

# groupby()에 배열 형태 전달하기
grouped_2 = diamonds['price'].groupby([diamonds['color'], diamonds['clarity']]).mean()
grouped_2

# 하위 인덱스였던 clarity를 컬럼으로 변환하기
grouped_2.unstack()

# cut 컬럼을 인덱스로 전체 데이터의 평균을 구하는 그룹 연산하기
diamonds.groupby(['cut']).mean()

# size() 함수만 적용하면 결과를 시리즈로 반환하지만 unstack() 함수와 결합하면 2개 축의 유일값을 기준으로 매칭한 개수를 데이터프레임으로 반환받을 수 있다.
diamonds.groupby(['cut', 'clarity']).size().unstack()
```

- groupby-apply 작업은 고급 데이터 변환 및 집계 작업, 피벗 테이블 관련 작업에 있어서 필수이다.
```python
# 데이터셋 로드
penguins = sns.load_dataset("penguins")
penguins = penguins.dropna()

# sort_values() 함수를 활용한 값을 리턴하는 top() 함수 작성하기
def top(data, n=5, column='body_mass_g'):
	return data.sort_values(by=column)[-n:]

# 디폴트로 상위 5개 값을 반환하지만 8개 값을 반환받기
top(penguins, n=8)

# sex 컬럼에 top() 함수를 apply하기
penguins.groupby('sex').apply(top)

# sex와 species 등 2개 컬럼을 기준으로 top() 함수 apply하기. 대신 행 수는 1개만 출력, body_mass_g 컬럼의 값 집계한다.
penguins.gruopby(['sex', 'species']).apply(top, n=1, column='body_mass_g')
```

<br/>

## **인상 깊었던 내용**
데이터 길이와 인덱스가 다른 데이터를 비교하기 위해서 전에 배웠던 내용인 <br/>
merge() 함수를 사용해야 한다는 것이 인상깊었다. 단순히 합치는 역할만 할 줄 알았던 것이 <br/>
인덱스가 다른 경우 동일한 값으로 인식시킬 때에도 사용된다는 점은 예상치 못했기 때문이다. <br/>
책에도 나왔듯이 merge() 함수를 활용해서 데이터 비교시키는 게 많이 까다로워서도 인상깊었다.

<br/>

## **느낀점**
갈수록 외워야할 코드들이 방대해져서 점점 뭘 배우고 있는지 혼란스러워지고 있다. <br/>
전에 배운 내용도 체화시키지 못했는데 꾸역꾸역 새로운 내용을 넣고 있자니 머리가 아파온다. <br/>
특히 groupby 너무 까다롭게 느껴진다. 너무 다양하게 사용될 수 있는 것 같아 보인다. <br/>
그럼에도 나중에 내게 도움이 될 것이라고 생각하니 스터디 활동이 뿌듯하게 느껴지기도 한다. <br/>
