## 주요내용
### 4.3 데이터 병합 후 처리
#### 합친 데이터에서 중복 행 확인 및 삭제하기
duplicated() 함수를 활용해 데이터의 중복 유무 확인
```python
data.duplicated(keep='first')
data.duplicated(keep='first').value_counts() -> 중복이 몇개인지 확인
data.duplicated(subset=['컬럼']).value_counts() -> 특정 컬럼의 중복 개수 확인
```
drop_duplicates() 함수를 활용해 데이터 삭제
```python
data.drop_duplicates()
data.drop_duplicates（subset=['패션아이템,, '스타일,], keep='last'） -> 매개변수 활용
```
#### 데이터 비교
compare() 함수를 활용해 값 차이 비교
```
data1.compare(data2)
data1.compare(data2, keep_equal=True) -> NaN 대신 원본 데이터 값을 보고싶을 때
data1.compare(data2, align_axis=0) -> 세로축으로 보기
```
#### 길이가 다른 데이터 비교
길이가 같은 경우는 compare() 함수를 사용하지만<br>
길이가 다른 경우는 eq() 함수를 사용해서 데이터 비교
```
data_e.eq(data_e_2) -> 긴 데이터를 기준으로 비교
data_e_2[data_e.eq(data_e_2).all(axis=l) == False] -> True에 해당하는 데이터만 필터링
```
#### 데이터 길이와 인덱스가 다른 데이터 비교
1. eq() 함수를 사용해 동일한 값 찾기
2. 인덱스가 다른 경우 동일한 값으로 변경 
3. merge() 함수로 다른 데이터 상태로 비교 가능

### 5.1 그룹 연산의 이해
#### groupby() 연산 기초
다이아몬드 데이터셋 활용
다이아몬드의 가격을 기준으로 투명도로 그룹화
```python
grouped = diamonds['price'].groupby(diamonds['clarity'])
grouped.mean()
```
평균이 아닌 다른 연산 함수도 적용 가능
```python
diamonds.groupby(['cut', 'clarity']).size().unstack()
```
size 함수만 적용하면 결과를 시리즈로 변환하지만 unstack() 함수와 결합하면 <br>
2개 축의 유일값을 기준으로 매칭한 개수를 반환 가능

#### 데이터 집계
```python
mapping{~~}
food.groupby(mapping, axis=l).sum() ->mapping 변수를 적용한 후 숫자의 합 구하기
mapping_series = pd.Series(mapping) -> mapping 변수를 시리즈 타입으로 변경
food.groupby(mapping_series, axis=l).count()
```
이 외에도 다양한 함수 적용 가능
#### groupby-apply 작업
```python
penguins = sns.load_dataset("penguins")
penguins = penguins.dropna()
def top(data, n=5, column='body_mass_g'): -> 5개의 값 추출하는 함수 생성
return data.sort_values(by=column)[-n:]
top(penguins, n=8) -> 8개의 값 반환
penguins.groupby('sex').apply(top)
```
describe() 함수를 활용하면 좀 더 상세한 분포 확인 가능

### 인상깊었던 내용
데이터의 중복 행의 여부를 확인하거나 그 값을 구하는 부분이 굉장히 인상깊었다.<br>
또한 중복된 데이터를 찾아서 중복된 데이터만 삭제하는 과정도 신기했다.<br>
groupby와 다양한 함수들을 이용해서 데이터를 집계하는 방법도 인상깊었다.

### 느낀점
지금까지 데이터 분석 스터디를 하면서 가장 어려운 내용이었다. <br>
특히 groupby 부분이 매우 어려워서 아직도 이해가 잘 안된다.<br>
하지만 앞으로 다양한 예제를 접해보고 groupby와 관련된 함수를 많이 사용하다보면 익숙해질 것이라고 생각한다.
